#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sun Feb 28 16:49:45 2021

@author: niegu
"""

import pandas as pd
import numpy as np
import sys
from tqdm import tqdm
from subprocess import Popen, PIPE, STDOUT
import subprocess
import os
import matplotlib.pyplot as plt
import random
import shutil
import stat
from functools import partial
import joblib
import copy
import networkx as nx
import re
import pickle
from Utilities.weighted_network import weighted_network
from Utilities.global_names import resources, facebook_network, communities, node2vec,\
 adaptive_temp, adaptive_code

def get_features_nodes(
    graph,
    dims=20,
    epochs=1,
    node2vec_path="node2vec",
    tempdir_name="temp_dir",
    dataset_name="facebook",
    check_existing=True,
):
    """ Generate node2vec features for the nodes in a graph

    Parameters
    ----------
    graph : networkx graph
        The graph we run node2vec on.
    dims : int, optional
        Specifies the number of dimensions to pass to node2vec[1]. Default: 20
    epochs : int, optional
        Specifies the number of epochs to run the node2vec for. Default: 1
    node2vec_path : str, optional
        Path to the node2vec executable. Default: "node2vec", which means that it is
        in one folder with this file
    tempdir_name : str, optional
        The name of a temporary directory to use for running node2vec.
        Default: "temp_dir"
    dataset_name : str, optional
        The name of the dataset, used when naming the files in the temporary directory.
        Default: "facebook"
    check_existing : boolean, optional
        Determines if we are checking for an alredy existing algorithm output in the
        temporary directory.


    Returns
    -------
    df_emb : pd.DataFrame
        A dataframe with the contents of embeddings generated by node2vec. It is also
        available as <dataset_name>-d<dims>.emb in the temporary directory.

    References
    ----------
    .. [1] Grover, Aditya, and Jure Leskovec. "node2vec: Scalable feature learning for networks."
        Proceedings of the 22nd ACM SIGKDD international conference on
        Knowledge discovery and data mining. 2016.

    """
    if not os.path.exists(tempdir_name):
        os.makedirs(tempdir_name)

    FNAME_IN = os.path.join(tempdir_name, f"{dataset_name}.edgelist")
    FNAME_OUT = os.path.join(tempdir_name, f"{dataset_name}-d{dims}.emb")

    # Checking if we already ran the function before
    if check_existing and os.path.exists(FNAME_OUT):
        return pd.read_csv(
            FNAME_OUT,
            sep=" ",
            names=(["node"] + [f"feat_{i}" for i in range(dims)]),
            skiprows=1,
        )
    # Saving the edgelist to run node2vec on
    nx.write_edgelist(graph, FNAME_IN)
    #TODO node2vec.exe won't run for me
    # Preparing to run node2vec
    process = Popen(
        [
            node2vec_path,
            f"-i:{FNAME_IN}",
            f"-o:{FNAME_OUT}",
            f"-d:{dims}",
            f"-e:{epochs}",
            "-v",
        ],
        shell=True,
        stdout=PIPE,
        stderr=PIPE,
    )
    (output, err) = process.communicate()
    _ = process.wait()  # Returns exit code

    return pd.read_csv(
        FNAME_OUT,
        sep=" ",
        names=(["node"] + [f"feat_{i}" for i in range(dims)]),
        skiprows=1,
    )


def generate_node2vec_fetures(
    graph,
    node2vec_path="node2vec.exe",
    tempdir_name="temp_dir",
    dataset_name="facebook",
    num_features=20,
    check_existing=True,
):
    """ Generate node2vec features for the edges in a graph

    Runs get_features_nodes to get the node2vec[1] features for the nodes and then
    multiplies them to get the edge features.
    Edge feature vector Fe = source_node_feats * target_node_feats.

    Parameters
    ----------
    graph: pnetworkx graph
        The graph we run node2vec on.
    node2vec_path : str, optional
        Path to the node2vec executable. Default: "os.getcwd()", which means that it is
        in one folder with this
    tempdir_name : str, optional
        The name of a temporary directory to use for running node2vec.
        Default: "temp_dir"
    dataset_name : str, optional
        The name of the dataset, used when naming the files in the temporary directory.
        Default: "facebook"
    dims : int, optional
        Specifies the number of dimensions to pass to node2vec[1]. Default: 20
    check_existing : boolean, optional
        Determines if we are checking for an alredy existing algorithm output in the
        temporary directory.

    Returns
    -------
    df_feats : pd.DataFrame
        A dataframe with edge embeddings, where one row represents one edge.

    References
    ----------
    .. [1] Grover, Aditya, and Jure Leskovec.
        "node2vec: Scalable feature learning for networks."
        Proceedings of the 22nd ACM SIGKDD international conference on
        Knowledge discovery and data mining. 2016.

    """
    # Checking if we did this before
    df = nx.to_pandas_edgelist(graph)[["source", "target"]]
    FNAME_SAVE = os.path.join(tempdir_name, f"{dataset_name}-d{num_features}-edges.emb")
    if check_existing and os.path.exists(FNAME_SAVE):
        df_ret = pd.read_csv(FNAME_SAVE)
        df_ret[df_ret.columns[0]] = df_ret[df_ret.columns[0]].astype(int)
        return df_ret.set_index(df_ret.columns[0])

    # Getting node embeddings
    
    df_emb = get_features_nodes(
        graph,
        dims=num_features,
        node2vec_path=node2vec_path,
        tempdir_name=tempdir_name,
        dataset_name=dataset_name,
        check_existing=check_existing,
    )
    df_emb = df_emb.set_index("node").sort_values(by="node")

    # Generating edge features
    df_feats = []
    for row in tqdm(df.itertuples(), total=df.shape[0]):
        df_feats.append(df_emb.loc[row.source].values * df_emb.loc[row.target].values)
    df_feats = pd.DataFrame(df_feats)

    # Saving the results
    df_feats.to_csv(FNAME_SAVE)

    return df_feats

def run_ic_eff(df_graph, seed_nodes):
    """ Simulate the influence propagation using the IC model

    Parameters
    ----------
    df_graph : pandas.DataFrame
        The graph we run the IC on, in the form of a DataFrame. A row represents one
        edge in the graph, with columns being named "source", "target", "act_prob".
        "act_prob" column contains the activation probability.
    seed_nodes : list, pandas.Series
        A list of the nodes to start propagating from.

    Returns
    -------
    results : tuple
        A tuple of the following numpy arrays
        - Affected nodes
        - Activated edges
        - Observed edges
    """    
    affected_nodes = copy.deepcopy(seed_nodes)  # copy already selected nodes
    activated_edges = []
    observed_edges = []
    df_graph["activated"] = df_graph["act_prob"].apply(lambda x: random.random() <= x)

    i = 0
    while i < len(affected_nodes):
        # for neighbors of a selected node
        for row in df_graph[df_graph["source"] == affected_nodes[i]].itertuples():
            observed_edges.append(row.Index)
            if row.activated and row.target not in affected_nodes:
                activated_edges.append(row.Index)
                affected_nodes.append(row.target)
        i += 1

    return np.array(affected_nodes), np.array(activated_edges), np.array(observed_edges)

def tim(
    df,
    num_nodes,
    num_edges,
    num_inf,
    epsilon,
    temp_dir="temp_dir",
    out_pattern=re.compile("Selected k SeedSet: (.+?) \\n"),
):
    """ Run the Offline IM algorithm, TIM

    Parameters
    ----------
    df : pandas.DataFrame
        The graph we run the TIM on, in the form of a DataFrame. A row represents one
        edge in the graph, with columns being named "source", "target", "act_prob".
        "act_prob" column contains the activation probability.
    num_nodes : int
        Number of nodes to pass into TIM.
    num_edges : int
        Number of edges to pass into TIM.
    num_inf : int
        Number of seed nodes to find.
    epsilon : float
        A hyperparameter for TIM. Refer to the paper for more details. [1]
    temp_dir : str, optional
        A temporary directory to run TIM in. Default: "temp_dir"
    out_pattern : re.Pattern, optional
        Regex pattern that gets the TIM results out of its output.
        Default: re.compile("Selected k SeedSet: (.+?) \\n"),

    Returns
    -------
    seeds : list
        A set of seed nodes that maximizes influence found by TIM

    References
    ----------
    .. [1] Tang, Youze, Xiaokui Xiao, and Yanchen Shi.
        "Influence maximization: Near-optimal time complexity meets practical efficiency."
        Proceedings of the 2014 ACM SIGMOD international conference on Management of data. 2014.
    """
    if not os.path.exists(temp_dir):
        os.makedirs(temp_dir)

    df.to_csv(
        os.path.join(temp_dir, "graph_ic.inf"), index=False, sep=" ", header=False
    )
    # Preparing to run TIM
    with open(os.path.join(temp_dir, "attribute.txt"), "w+") as f:
        f.write(f"n={num_nodes}\nm={num_edges}")
    #TODO Check about how this works with tim not checking for arguments
    tim_path = os.path.join(adaptive_code,"tim");
    process = Popen(
        [
            tim_path,
            "-model",
            "IC",
            "-dataset",
            temp_dir,
            "-k",
            f"{num_inf}",
            "-epsilon",
            f"{epsilon}",
        ],
        #shell = True,
        stdout=PIPE,
        stderr=PIPE
        #stderr=STDOUT
    )
    (output, err) = process.communicate()
    _ = process.wait()  # Returns exit code
    out = output.decode("utf-8")
    # logging.debug(f"Running TIM, {out}")
    return list(map(int, out_pattern.findall(out)[0].split(" ")))

def imlinucb_node2vec(
    G,
    df_feats,
    num_inf,
    num_repeats,
    epsilon=0.4,
    sigma=4,
    c=0.1,
    tempdir_name="temp_dir"
):
    """ Test of the Online IM with Node2Vec features

    Online Influence Maximization using IMLinUCB [1] with node2vec [3] features. The
    "test" version also compares the result obtained to the "true" seed set, generated
    by an Offline IM algorithm with the actual activation probabilities. This uses the
    node2vec node embeddings to speed up the processing.

    Parameters
    ----------
    df : pandas.DataFrame
        The graph we run the OIM on, in the form of a DataFrame. A row represents one
        edge in the graph, with columns being named "source", "target", and "act_prob".
        "act_prob" column is the "true" activation probability.
    df_feats : pandas.DataFrame
        A dataframe with node embeddings generated by node2vec [3]. Should have n rows,
        where n is the number of nodes in the graph. The number of columns is specified
        when running node2vec on the graph. The smaller the number of columns is the
        more uncertain the embedding, but it also speeds up processing.
    num_inf : int, optional
        Number of seed nodes to find. Default: 10.
    sigma : float, optional
        A parameter used by the IMLinUCB algorithm. Refer to the IMLinUCB paper for
        more details. [1] Default: 4
    c: float, optional
        A parameter used by the IMLinUCB algorithm. Refer to the IMLinUCB paper for
        more details. [1] Default: 0.1
    epsilon : float, optional
        A parameter used by the TIM algorithm. Refer to the TIM paper for
        more details. [2] Default: 0.4
    num_repeats : int, optional
        Number of iterations of the IMLinUCB algorithm. The more, the better the
        results. 

    Returns
    -------
    return_dict : dict
        A dictionary consisting of following keys:
        - rewards, a list of all rewards (number of inf nodes) obtained by the algorithm
        - rewards_edges, a list of edge rewards (number of inf edges) obtained
        - s_best, the list of the selected seed nodes
        - u_e_best, the approximated probabilities
        - reward_best, the reward obtained by running IC with s_best
    """
    # Use 0.1 if no "act_prob" provided
#    for i, j in G.edges():
#        if G[i][j].get("act_prob") is None:
#            G[i][j]["act_prob"] = 0.1
    
    df = nx.to_pandas_edgelist(G)[["source", "target", "act_prob"]]
    
    #print(df_feats)
    num_feats = df_feats.shape[1]
    num_edges_t = df.shape[0]

    # "True" probabilities - effectively our test set
    true_weights = df["act_prob"].copy()
    
    #s_true = sorted(tim.NodeSelection(G, num_inf))
    
    # Gathering the stats for the "true" seed set
#    all_true_nodes = []
#    all_true_edges = []
#    all_true_obs = []
#    for k in range(num_repeats_regret_true):
#        true_act_nodes, true_act_edges, true_obs_edges = run_ic_eff(df, s_true)
#        all_true_nodes.append(true_act_nodes)
#        all_true_edges.append(true_act_edges)
#        all_true_obs.append(true_obs_edges)

    # Means for nodes and activated edges
    #mean_true_nodes = np.mean([len(i) for i in all_true_nodes])

    # b, M_inv - used by IMLinUCB
    b = np.zeros((num_feats, 1))
    m_inv = np.eye(num_feats, num_feats)

    # Returning these
    s_best = []
    reward_best = 0
    u_e_best = []
    rewards = []
    #regrets = []
    seed_sets = []
    
    #new dir randint
    rand = random.randint(0, 1000000)
    
    for iter_oim in tqdm(
        range(num_repeats),
        desc=f"OIM iters {num_edges_t} edges",
        leave=False,
        file=sys.stderr,
    ):
        # ---- Step 1 - Calculating the u_e ----
        theta = (m_inv @ b) / (sigma * sigma)
        # xMx = (df_feats.values @ m_inv @ df_feats.T.values).clip(min=0)

        u_e = []
        for i in range(num_edges_t):
            x_e = df_feats.loc[i].values
            xMx = x_e @ m_inv @ x_e.T  # .clip(min=0)
            u_e.append(np.clip(x_e @ theta + c * np.sqrt(xMx), 0, 1))
            # u_e.append(expit(x_e @ theta + c * np.sqrt(xMx)))

        u_e = np.array(u_e)

        # ---- Step 2 - Evaluating the performance ----
        # Loss function
        df["act_prob"] = u_e
        
        #G1 = nx.from_pandas_edgelist(df, "source", "target", edge_attr="act_prob", create_using=nx.DiGraph)
        s_oracle = sorted(
            tim(
                df[["source", "target", "act_prob"]],
                len(G.nodes()),
                num_edges_t,
                num_inf,
                epsilon,
                temp_dir=tempdir_name + str(rand)
            )
        )
        #print(s_oracle)
        
        # Observing edge-level feedback
        df["act_prob"] = true_weights

        algo_act_nodes, algo_act_edges, algo_obs_edges = run_ic_eff(df, s_oracle)
        seed_sets.append(s_oracle)
        rewards.append(len(algo_act_nodes))
        #regrets.append(mean_true_nodes - len(algo_act_nodes))
        
        if len(algo_act_nodes) > reward_best:
            reward_best = len(algo_act_nodes)
            s_best = s_oracle
            u_e_best = u_e
            
        # ---- Step 3 - Calculating updates ----
        for i in algo_obs_edges:
            x_e = np.array([df_feats.loc[i].values])
            m_inv -= (m_inv @ x_e.T @ x_e @ m_inv) / (
                x_e @ m_inv @ x_e.T + sigma * sigma
            )
            b += x_e.T * int(i in algo_act_edges)

    return_dict = {
        "rewards": rewards,
        "seed_sets": seed_sets,
        #"regrets": regrets,
        #"s_true": s_true,
        "s_best": s_best,
        "u_e_best": u_e_best,
        "reward_best": reward_best,
    }
    return return_dict

def main():
    """----------------------------------"""
    """READING/INTIALIZING THE NETWORK"""
    """Working with the Facebook network """
    "Reading the Facebook network from file"
    
    facebook_path = os.path.join(resources, facebook_network)
    network = nx.read_edgelist(facebook_path,create_using=nx.DiGraph(), nodetype = int)
    
    "Reading communities"
    communities_path = os.path.join(resources, communities)
    with open(communities_path, 'rb') as f:
        part = pickle.load(f)
    value = [part.get(node) for node in network.nodes()]
    nodes_subset = [key for key,value in part.items() if value == 4]
       
    "Working with a subgraph after deleting an outlier node"
    nodes_subset.remove(1684)
    network = nx.subgraph(network, nodes_subset).copy() # .copy() makes a subgraph with its own copy of the edge/node attributes
    
    """Working with the Florentine families network """
    "Declaring the Florentine families network"
    #network = nx.florentine_families_graph()
    #network = nx.karate_club_graph()
    """----------------------------------"""
    
    "Converting the subgraph to a directed graph"
    network = network.to_directed()
    
    "Relabeling the nodes as positive integers viz. 1,2,..."
    network = nx.convert_node_labels_to_integers(network,first_label=0)
    
    network = weighted_network(network, method='wc')
    
    temp_directory = os.path.join(adaptive_temp, "ImLinUCB_temp")
    
    node_path = os.path.join(resources, node2vec)
    df_feats = generate_node2vec_fetures(network, node2vec_path = node_path, tempdir_name=temp_directory,
                                         dataset_name = "facebook")
    
    #oracle = tim(df, len(network.edges()), len(df), 5, 0.4)
    dic = imlinucb_node2vec(G=network, df_feats=df_feats, num_inf=5, num_repeats=10,tempdir_name=temp_directory)
    
    #print(dic)
    #tim(df, num_nodes, num_edges, num_inf, epsilon)
